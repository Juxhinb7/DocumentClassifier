Researchers have found a better way to reduce gender bias in natural language processing models while preserving vital information about the meanings of words, according to a recent study that could be a key step toward addressing the issue of human biases creeping into artificial intelligence.

While a computer itself is an unbiased machine, much of the data and programming that flows through computers is generated by humans. This can be a problem when conscious or unconscious human biases end up being reflected in the text samples AI models use to analyze and "understand" language.

Computers aren't immediately able to understand text, explains Lei Ding, first author on the study and graduate student in the Department of Mathematical and Statistical Sciences. They need words to be converted into a set of numbers to understand themâ€”a process called word embedding.

"Natural language processing is basically teaching the computers to understand texts and languages," says Bei Jiang, associate professor in the Department of Mathematical and Statistical Sciences.

Once researchers take this step, they're able to then plot words as numbers on a 2D graph and visualize the words' relationships to one another. This allows them to better understand the extent of the gender bias, and later, determine whether the bias was effectively eliminated.

Though other attempts to reduce or remove gender bias in texts have been successful to some degree, the problem with those approaches is that gender bias isn't the only thing removed from the texts.

"In many gender debiasing methods, when they reduce the bias in a word vector, they also reduce or eliminate important information about the word," explains Jiang. This type of information is known as semantic information, and it offers important contextual data that could be needed in future tasks involving those word embeddings.

For example, when considering a word like "nurse," researchers want the system to remove any gender information associated with that term while still retaining information that links it with related words such as doctor, hospital and medicine.

"We need to preserve that semantic information," says Ding. "Without it, the embeddings would have very bad performance [in natural language processing tasks and systems]."