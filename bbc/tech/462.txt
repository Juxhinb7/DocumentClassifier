With the introduction of DALL-E, the internet had a collective feel-good moment. This artificial intelligence-based image generator is inspired by artist Salvador Dali and the lovable robot WALL-E and uses natural language to produce whatever mysterious and beautiful image your heart desires. Seeing typed-out inputs such as “smiling gopher holding an ice cream cone” instantly spring to life is a vivid AI-generated image clearly resonated with the world.

A sample DALL·E 2 generated image of “an astronaut riding a horse in a photorealistic style.” Credit: Open AI

A new method developed by researchers uses multiple models to create more complex images with better understanding.

With the introduction of DALL-E, the internet had a collective feel-good moment. This artificial intelligence-based image generator is inspired by artist Salvador Dali and the lovable robot WALL-E and uses natural language to produce whatever mysterious and beautiful image your heart desires. Seeing typed-out inputs such as “smiling gopher holding an ice cream cone” instantly spring to life is a vivid AI-generated image clearly resonated with the world.

It is not a small task to get said smiling gopher and attributes to pop up on your screen. DALL-E 2 uses something called a diffusion model, where it tries to encode the entire text into one description to generate an image. However, once the text has a lot more details, it’s hard for a single description to capture it all. Furthermore, while they’re highly flexible, diffusion models sometimes struggle to understand the composition of certain concepts, such as confusing the attributes or relations between different objects.

To generate more complex images with better understanding, scientists from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) structured the typical model from a different angle: they added a series of models together, where they all cooperate to generate desired images capturing multiple different aspects as requested by the input text or labels. To create an image with two components, say, described by two sentences of description, each model would tackle a particular component of the image.

The seemingly magical models behind image generation work by suggesting a series of iterative refinement steps to get to the desired image. It starts with a “bad” picture and then gradually refines it until it becomes the selected image. By composing multiple models together, they jointly refine the appearance at each step, so the result is an image that exhibits all the attributes of each model. By having multiple models cooperate, you can get much more creative combinations in the generated images.

Take, for example, a red truck and a green house. When these sentences get very complicated, the model will confuse the concepts of red truck and green house. A typical generator like DALL-E 2 might swap those colors around and make a green truck and a red house. The team’s approach can handle this type of binding of attributes with objects, and especially when there are multiple sets of things, it can handle each object more accurately.

“The model can effectively model object positions and relational descriptions, which is challenging for existing image-generation models. For example, put an object and a cube in a certain position and a sphere in another. DALL-E 2 is good at generating natural images but has difficulty understanding object relations sometimes,” says Shuang Li, MIT CSAIL PhD student and co-lead author. “Beyond art and creativity, perhaps we could use our model for teaching. If you want to tell a child to put a cube on top of a sphere, and if we say this in language, it might be hard for them to understand. But our model can generate the image and show them.”